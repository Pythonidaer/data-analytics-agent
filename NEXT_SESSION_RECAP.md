# Next AI Agent Session - Comprehensive Recap & Handoff

> **Purpose:** This document provides comprehensive context for the next AI agent session to seamlessly continue the domain conversion work.

**Date Prepared:** December 26, 2025  
**Session Goal:** Begin Exercise 5 (Domain Conversion) - PMM Agent ‚Üí Data Analytics Expert Agent  
**Starting Point:** Original fork completed through Exercise 4, now creating new repo for custom domain

---

## 1. Project Context & Background

### What This Project Is

This is the **Jai's Agent Accelerator** project - a production-ready AI agent framework that uses the Mixture of Experts (MoE) pattern. The original project is a Product Marketing Manager (PMM) agent that helps product marketers create positioning statements, battlecards, messaging matrices, and launch plans.

**Repository:** `jai-agent-accelerator` (forked from `github.com/ChaiWithJai/jai-agent-accelerator`)

### What We're Doing

We're completing **Exercise 5: Domain Conversion** to transform the PMM agent into a **Data Analytics Expert Agent**. This is part of an internship application process where the user needs to:

1. ‚úÖ Complete all exercises (Exercises 1-4 are done)
2. ‚úÖ Deploy to production (completed, using Vercel)
3. ‚ö†Ô∏è **Convert to custom domain** (this is what we're starting)
4. ‚ö†Ô∏è Help someone else get it running (future)
5. ‚ö†Ô∏è Create teaching documentation (future)

### Why Two Repositories?

- **Original fork (`jai-agent-accelerator`)**: Keeps the PMM agent intact for reference/comparison
- **New repo (to be created)**: Will contain the Data Analytics domain conversion - this is what we'll work on

---

## 2. Current Project State

### What's Been Completed

‚úÖ **Exercise 1**: Hello, Agent - Got agent running locally  
‚úÖ **Exercise 2**: Prompt Surgery - Added clarification protocol  
‚úÖ **Exercise 3**: Build Your First Tool - Custom tools working  
‚úÖ **Exercise 4**: Deploy to Production - Deployed to Vercel at `https://my-pmm-agent.vercel.app`

‚úÖ **Production Checklist**: Mostly complete
- API Key Security
- CORS Configuration (restricted to Vercel URL)
- Rate Limiting (slowapi implementation)
- Input Validation (Pydantic, 1-50k chars)
- HTTPS Only (automatic on Vercel)
- Response Caching (health & metrics endpoints)
- Conversation Truncation (MAX_MESSAGE_HISTORY=100)
- Model Selection (using Sonnet 4 globally, decision documented)
- Cold Start Optimization (decision made: not implementing)
- Error Tracking (using Vercel logs)
- Health Checks (endpoint exists)
- Usage Metrics (basic endpoint exists)

‚úÖ **Infrastructure**: All working
- FastAPI backend with LangGraph ReAct agent
- React frontend (Vite + TypeScript)
- Streaming responses
- Tool calling (ReAct agent enforces tool usage)
- Observability logging
- Deployed to Vercel production

### What We're Starting Now

‚ö†Ô∏è **Exercise 5: Domain Conversion** - Convert PMM ‚Üí Data Analytics Expert Agent

**Target Domain:** Data Analytics (Product & Business Analytics focus)

**Scope:**
- Metrics design and KPI definitions
- Event tracking plans
- SQL query templates (funnels, cohorts, retention)
- Dashboard specifications
- Data quality checks and validation

---

## 3. Repository Structure Understanding

### Current Architecture

```
jai-agent-accelerator/                    # Original fork (PMM domain)
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ index.py                          # Vercel adapter (imports server:app)
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt                  # Production dependencies
‚îú‚îÄ‚îÄ apps/
‚îÇ   ‚îú‚îÄ‚îÄ agent/                            # Python backend
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ src/pmm_agent/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server.py                 # FastAPI app, streaming, sessions
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent.py                  # Agent factory (create_pmm_agent)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts.py                # PMM system prompt + specialist prompts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ observability.py          # Logging infrastructure
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tools/                    # PMM tools organized by phase
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py           # Exports ALL_TOOLS
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ intake.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ research.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ planning.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ risk.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ scoring.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml                # Python dependencies
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tests/                        # Test scripts (organized)
‚îÇ   ‚îú‚îÄ‚îÄ web/                              # React frontend
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ App.tsx                   # Main component, QUICK_ACTIONS hardcoded
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îî‚îÄ‚îÄ domains/
‚îÇ           ‚îî‚îÄ‚îÄ pmm.json                  # PMM domain config (existing)
‚îú‚îÄ‚îÄ docs/                                 # Comprehensive documentation
‚îÇ   ‚îú‚îÄ‚îÄ EXERCISES.md                      # Exercise 5 requirements
‚îÇ   ‚îú‚îÄ‚îÄ CUSTOMIZATION.md                  # 4-step domain conversion guide
‚îÇ   ‚îú‚îÄ‚îÄ METHODOLOGY.md                    # MoE methodology explained
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT.md                     # Deployment instructions
‚îÇ   ‚îî‚îÄ‚îÄ ... (many more docs)
‚îî‚îÄ‚îÄ vercel.json                           # Vercel configuration
```

### Key Technologies

- **Backend:** Python 3.11+, FastAPI, LangChain, LangGraph, Anthropic Claude API
- **Frontend:** React, TypeScript, Vite, Tailwind CSS
- **Deployment:** Vercel (serverless functions)
- **Agent Pattern:** Mixture of Experts (MoE) with ReAct agent from LangGraph

### Important Code Patterns

1. **Agent Factory Pattern** (`agent.py`):
   - `create_pmm_agent(mode="full", model_name=...)` - Creates ReAct agent
   - Takes system prompt from `prompts.py`
   - Selects tools based on mode (full/intake/research/planning/risk)
   - Uses LangGraph's `create_react_agent` for tool calling enforcement

2. **Tool Pattern** (`tools/*.py`):
   - Tools are `@tool` decorated functions from `langchain_core.tools`
   - Clear docstrings (Claude reads these to decide when to call)
   - Return structured outputs (Pydantic models or Markdown)
   - Organized by workflow phase

3. **Streaming Pattern** (`server.py`):
   - `/chat/stream` endpoint uses LangGraph agent's `astream` method
   - Processes LangGraph events to extract tool calls and text
   - Emits SSE (Server-Sent Events) to frontend
   - Tool calls are deduplicated before streaming

---

## 4. Domain Conversion Plan (Reference)

### Comprehensive Gameplan Document

**File:** `DOMAIN_CONVERSION_GAMEPLAN.md` (in current repo, will copy to new repo)

This is the **master document** with:
- Complete conversion strategy
- File-by-file change list
- Domain configuration JSON structure
- Analytics prompt templates
- Tool implementation patterns
- Frontend quick actions updates
- Testing strategy
- 2-hour implementation timeline

**Key Sections:**
- Section 3: Domain Configuration File (`config/domains/data_analytics.json`)
- Section 4: Domain Prompts (how to rewrite for analytics)
- Section 5: Analytics Tools (5+ custom tools needed)
- Section 6: Wiring It Up (code changes)
- Section 7: Frontend Quick Actions
- Section 8: 2-Hour Conversion Timeline
- Section 17: Alignment with Official Documentation

### Exercise 5 Requirements (from `docs/EXERCISES.md`)

1. Create new domain config: `config/domains/data_analytics.json`
2. Identify 3-5 experts for domain (we have 4: Analytics Strategist, Product Analyst, Analytics Engineer, Insights Communicator)
3. Create 3+ custom tools (we're planning 5+)
4. Write domain-specific prompts with expert knowledge
5. Update frontend quick actions
6. Deploy the customized agent

**Success Criteria:**
- Agent responds with domain-specific expertise
- At least 3 custom tools working (visible in UI)
- Quick actions match domain
- Can explain the "Giants" (frameworks) for domain

### The "Giants" (Frameworks) for Analytics

1. **Kimball Dimensional Modeling** - Facts vs dimensions, star schemas
2. **North Star Metric + KPI Trees** - Metric hierarchies
3. **Cohort / Funnel / Retention Analysis** - Product growth measurement
4. **Experimentation & Causal Thinking** - A/B testing, statistical validity
5. **Modern Analytics Engineering (dbt-style)** - Data tests, lineage

---

## 5. Files to Copy to New Repository

### Planning Documents (Currently in `.gitignore`)

These files exist locally but won't be in a fresh clone. They need to be **manually copied** to the new repo:

1. `DOMAIN_CONVERSION_GAMEPLAN.md` - **CRITICAL** - Master conversion plan
2. `TOMORROW_GAMEPLAN.md` - Implementation plan for tomorrow (may be outdated)
3. `INTERNSHIP_APPLICATION.md` - Application tracking
4. `docs/ERROR_TRACKING_ANALYSIS.md` - Error tracking decision rationale
5. `COPY_TO_NEW_REPO.md` - Instructions for copying files

### What's Already Tracked (Will Be in Clone)

- All source code
- All documentation in `docs/`
- Configuration files
- `.gitignore` (already has planning docs excluded)
- **`NEXT_SESSION_RECAP.md`** (this file - will be in clone!)

---

## 6. Session Flow & Tasks

### Phase 1: Repository Setup (First Priority)

**Task:** Create new repository and copy necessary files

1. **Clone current repo to new directory:**
   ```bash
   cd ~/Documents
   cp -r jai-agent-accelerator data-analytics-agent  # or your chosen name
   cd data-analytics-agent
   ```

2. **Initialize new git repo:**
   ```bash
   rm -rf .git
   git init
   git add .
   git commit -m "Initial commit: Data Analytics domain conversion"
   ```

3. **Create new GitHub repository** (user will do this, or guide them)

4. **Connect to new GitHub repo:**
   ```bash
   git remote add origin https://github.com/YOUR-USERNAME/data-analytics-agent.git
   git branch -M main
   git push -u origin main
   ```

5. **Manually copy planning documents:**
   ```bash
   # From OLD repo directory
   cp DOMAIN_CONVERSION_GAMEPLAN.md ../data-analytics-agent/
   cp INTERNSHIP_APPLICATION.md ../data-analytics-agent/
   cp docs/ERROR_TRACKING_ANALYSIS.md ../data-analytics-agent/docs/
   # etc.
   ```

6. **Update README.md** for new project (change name, description, URLs)

### Phase 2: Local Verification (Before Conversion)

**Task:** Ensure everything works before starting conversion

1. **Test current setup works:**
   ```bash
   cd apps/agent
   source .venv/bin/activate
   export ANTHROPIC_API_KEY=sk-ant-your-key
   python3 -m uvicorn src.pmm_agent.server:app --port 8123
   ```

2. **In another terminal, test frontend:**
   ```bash
   cd apps/web
   npm install  # May need to reinstall if dependencies changed
   npm run dev
   ```

3. **Verify:**
   - Backend starts without errors
   - Frontend loads at `http://localhost:3003`
   - Can have a conversation with PMM agent
   - Tool calls appear in UI
   - Health endpoint works: `curl http://localhost:8123/health`

### Phase 3: Domain Conversion Implementation

**Task:** Follow `DOMAIN_CONVERSION_GAMEPLAN.md` step-by-step

**Reference:** Section 8 (2-Hour Timeline) and Section 16 (Next Steps)

**Key Steps:**

1. **Create directory structure:**
   ```bash
   mkdir -p config/domains
   mkdir -p apps/agent/src/pmm_agent/domains/data_analytics/tools
   ```

2. **Create domain config** (`config/domains/data_analytics.json`):
   - Use template from `DOMAIN_CONVERSION_GAMEPLAN.md` Section 3
   - Includes: experts, tools by phase, frameworks, quick_actions

3. **Implement analytics prompts**:
   - Create `apps/agent/src/pmm_agent/domains/data_analytics/prompts.py`
   - Main system prompt + 4 specialist prompts
   - Follow structure from `docs/CUSTOMIZATION.md` Step 2
   - Reference PMM prompts in `apps/agent/src/pmm_agent/prompts.py` as template

4. **Implement analytics tools**:
   - Create tools in `domains/data_analytics/tools/`
   - At least 3 tools (we're planning 5+)
   - Follow pattern from existing PMM tools
   - Use `@tool` decorator, clear docstrings, structured outputs

5. **Wire up domain loader**:
   - Create `apps/agent/src/pmm_agent/domain_loader.py`
   - Load domain config JSON
   - Import domain-specific prompts/tools
   - Return system prompt + tools for agent creation

6. **Update agent factory** (`agent.py`):
   - Add `create_domain_agent(domain=..., mode=..., model_name=...)`
   - Keep `create_pmm_agent()` for compatibility
   - Use domain loader to get prompts/tools

7. **Update server** (`server.py`):
   - Use `DOMAIN` environment variable (defaults to "pmm")
   - Create agent based on domain selection
   - Keep backward compatibility

8. **Update frontend** (`apps/web/src/App.tsx`):
   - Replace `QUICK_ACTIONS` array with analytics actions
   - Update title/tagline/description
   - Use analytics-appropriate icons from lucide-react

### Phase 4: Testing & Iteration

**Task:** Test conversion incrementally

1. **Test domain switching locally:**
   ```bash
   export DOMAIN=data_analytics
   # Restart backend
   # Verify analytics quick actions appear
   # Test demo prompts from gameplan
   ```

2. **Verify tools work:**
   - Use demo prompts from `DOMAIN_CONVERSION_GAMEPLAN.md` Section 9
   - Verify tool calls appear in UI
   - Verify tool outputs are structured and useful

3. **Test backward compatibility:**
   ```bash
   export DOMAIN=pmm
   # Restart backend
   # Verify PMM agent still works
   ```

### Phase 5: Deployment to Vercel

**Task:** Deploy analytics domain to production

1. **Set environment variables:**
   ```bash
   vercel env add DOMAIN production
   # Enter: data_analytics
   
   vercel env add ANTHROPIC_API_KEY production
   # Enter API key (if not already set)
   ```

2. **Deploy:**
   ```bash
   vercel --prod
   ```

3. **Verify:**
   ```bash
   curl https://your-new-app.vercel.app/api/health
   # Test in browser
   # Verify analytics quick actions
   # Test tool calls
   ```

---

## 7. Important Patterns & Conventions

### Domain Conversion Philosophy

**This is a conversion, NOT a rewrite:**
- Keep all infrastructure (FastAPI, LangGraph, streaming, observability)
- Only change: prompts, tools, quick actions
- Maintain backward compatibility with PMM domain
- Follow existing code patterns and structure

### Mixture of Experts (MoE) Methodology

**Reference:** `docs/METHODOLOGY.md`

1. **Domain Decomposition** - Break analytics into specialized experts
2. **Semantic Grounding** - Ground in established frameworks (Kimball, North Star, etc.)
3. **Human-in-the-Loop** - Surface critical decisions
4. **Eval-Driven** - Measure and improve
5. **Iterative Refinement** - Start rough, improve through feedback

### Tool Design Principles

**Reference:** `docs/CUSTOMIZATION.md` Step 3

1. Clear Purpose - Each tool does one thing well
2. Typed Inputs - Use Pydantic models for complex inputs
3. Structured Outputs - Return typed objects, not raw strings
4. Good Docstrings - Agent uses these to decide when to call
5. Sensible Defaults - Make common cases easy

### Code Structure Convention

- Tools organized by phase: `intake/`, `research/`, `planning/`, `risk/`
- Prompts: Main system prompt + specialist prompts per expert
- Domain isolation: New domain in `domains/data_analytics/` directory
- Backward compatible: PMM domain continues to work

---

## 8. Key Files & Their Purposes

### Must-Read Files Before Starting

1. **`DOMAIN_CONVERSION_GAMEPLAN.md`** - Master plan (copy this first!)
2. **`docs/CUSTOMIZATION.md`** - Official 4-step conversion guide
3. **`docs/EXERCISES.md`** - Exercise 5 requirements (lines 319-401)
4. **`docs/METHODOLOGY.md`** - MoE methodology understanding
5. **`apps/agent/src/pmm_agent/prompts.py`** - PMM prompt structure (template)
6. **`apps/agent/src/pmm_agent/tools/intake.py`** - Tool pattern example
7. **`apps/agent/src/pmm_agent/agent.py`** - Agent factory pattern
8. **`apps/web/src/App.tsx`** - Frontend quick actions (lines 54-104)

### Files We'll Create

1. `config/domains/data_analytics.json` - Domain configuration
2. `apps/agent/src/pmm_agent/domain_loader.py` - Domain config loader
3. `apps/agent/src/pmm_agent/domains/data_analytics/prompts.py` - Analytics prompts
4. `apps/agent/src/pmm_agent/domains/data_analytics/tools/` - Analytics tools

### Files We'll Modify

1. `apps/agent/src/pmm_agent/agent.py` - Add `create_domain_agent()`
2. `apps/agent/src/pmm_agent/server.py` - Use DOMAIN env var
3. `apps/web/src/App.tsx` - Update quick actions
4. `README.md` - Update for analytics domain

### Files We Won't Touch

- `api/index.py` - Vercel adapter (no domain logic)
- `apps/agent/src/pmm_agent/observability.py` - Infrastructure
- `apps/agent/src/pmm_agent/tools/` - Keep PMM tools (for pmm domain)
- `vercel.json` - Deployment config (works as-is)

---

## 9. Environment Variables Reference

### Required (Local & Production)

```bash
ANTHROPIC_API_KEY=sk-ant-your-key-here  # Required for Claude API
```

### New (For Domain Conversion)

```bash
DOMAIN=data_analytics  # Options: "pmm" (default) or "data_analytics"
```

### Optional (Already Configured)

```bash
MODEL=claude-sonnet-4-20250514  # Default model
MAX_MESSAGE_HISTORY=100  # Conversation truncation limit
LOG_LEVEL=INFO  # Logging verbosity
ALLOWED_ORIGINS=https://your-app.vercel.app  # CORS (production)
```

---

## 10. Testing Strategy

### Local Testing Checklist

- [ ] Backend starts without errors
- [ ] Frontend loads and displays analytics quick actions
- [ ] Can have conversation with analytics agent
- [ ] Tool calls appear in UI when using demo prompts
- [ ] Tool outputs are structured and useful
- [ ] Domain switching works (pmm ‚Üî data_analytics)
- [ ] Health endpoint returns OK
- [ ] No console errors in browser

### Demo Prompts (From Gameplan Section 9)

These should reliably trigger analytics tools:

1. "We launched a new onboarding flow. Define the KPIs and create a metrics dictionary."
2. "Create a tracking plan for sign-up ‚Üí activation ‚Üí first value. Include events and properties."
3. "Draft SQL for activation funnel and 7-day retention. Assume events table with user_id, event_name, occurred_at."
4. "Create a data quality checklist for DAU and retention. What could go wrong?"

### Production Testing Checklist

- [ ] Deployed to Vercel successfully
- [ ] Environment variables set correctly
- [ ] Health endpoint returns OK
- [ ] Analytics quick actions appear
- [ ] Can have conversation through web interface
- [ ] Tool calls work in production
- [ ] HTTPS enabled (automatic)
- [ ] CORS configured correctly

---

## 11. Common Pitfalls to Avoid

‚ùå **Don't rewrite infrastructure** - Keep FastAPI, LangGraph, streaming as-is  
‚ùå **Don't break PMM domain** - Maintain backward compatibility  
‚ùå **Don't overcomplicate** - Simple domain loader is better than complex routing  
‚ùå **Don't forget tool docstrings** - Claude reads these to decide when to call tools  
‚ùå **Don't skip testing** - Verify tools work in UI before deploying  
‚ùå **Don't forget domain config** - Must create `config/domains/data_analytics.json`  
‚ùå **Don't ignore existing patterns** - Follow PMM tool/prompt structure  

‚úÖ **Do follow existing code patterns**  
‚úÖ **Do test incrementally** - Don't convert everything at once  
‚úÖ **Do reference official docs** - `docs/CUSTOMIZATION.md` is your guide  
‚úÖ **Do keep it simple** - MVP first, refine later  
‚úÖ **Do verify tool calls** - They must be visible in UI  

---

## 12. Success Criteria (Exercise 5)

From `docs/EXERCISES.md` lines 343-348:

- [ ] Agent responds with domain-specific expertise
- [ ] At least 3 custom tools working (visible in UI)
- [ ] Quick actions match your domain
- [ ] Can explain the "Giants" (frameworks) for your domain
- [ ] Deployed to production (Vercel)
- [ ] Health endpoint returns OK

---

## 13. Quick Reference Commands

### Local Development

```bash
# Backend
cd apps/agent
source .venv/bin/activate
export ANTHROPIC_API_KEY=sk-ant-your-key
export DOMAIN=data_analytics  # or "pmm" for original
python3 -m uvicorn src.pmm_agent.server:app --port 8123

# Frontend (new terminal)
cd apps/web
npm run dev

# Test health endpoint
curl http://localhost:8123/health
```

### Deployment

```bash
# Set environment variables
vercel env add DOMAIN production  # Enter: data_analytics

# Deploy
vercel --prod

# Check logs
vercel logs https://your-app.vercel.app
```

### Testing Tools

```bash
# From apps/agent directory
python3 tests/test_custom_tools.py  # Test tools work
python3 tests/run_deployment_checklist_test.py  # Verify checklist items
```

---

## 14. What the User Wants

### Immediate Goals

1. **Clone repo and set up new repository** - Clean start for analytics domain
2. **Get new repo working locally** - Verify PMM agent still works before conversion
3. **Convert to analytics domain** - Follow gameplan step-by-step
4. **Test incrementally** - Verify each change works before moving on
5. **Deploy to Vercel** - Get analytics agent live in production

### Long-term Goals (Future Sessions)

- Help someone else get it running (Exercise 5 requirement)
- Create teaching documentation
- Complete internship application

### User Preferences

- **Approach:** Methodical, test-as-you-go
- **Documentation:** Comprehensive (they appreciate detailed docs)
- **Quality:** Working and tested before moving forward
- **Clarity:** Understand what's happening at each step

---

## 15. Getting Started Checklist (For Next Agent)

When the next agent session starts, here's what to do:

1. ‚úÖ **Read this recap document** (you're doing it!)
2. ‚úÖ **Read `DOMAIN_CONVERSION_GAMEPLAN.md`** (after user copies it)
3. ‚úÖ **Understand the user wants to start with repository setup**
4. ‚úÖ **Follow Phase 1: Repository Setup** (Section 6 above)
5. ‚úÖ **Then Phase 2: Local Verification** (ensure PMM agent works)
6. ‚úÖ **Then Phase 3: Domain Conversion** (follow gameplan)
7. ‚úÖ **Ask questions if unclear** - User appreciates clarity
8. ‚úÖ **Test incrementally** - Don't do everything at once
9. ‚úÖ **Reference official docs** - `docs/CUSTOMIZATION.md` is authoritative

---

## 16. Key Context for Next Agent

### Important Notes

- **This is a conversion, not a rewrite** - Keep infrastructure intact
- **Backward compatibility matters** - PMM domain should still work
- **Follow existing patterns** - Don't invent new architectures
- **Test as you go** - Incremental verification prevents big problems
- **User is methodical** - They want to understand each step
- **Documentation is valued** - Clear explanations are appreciated

### Communication Style

- Be clear about what you're doing and why
- Reference specific file paths and line numbers when helpful
- Explain patterns before implementing
- Confirm understanding before making big changes
- Ask if uncertain - user prefers clarity over assumptions

### Technical Constraints

- Must work with existing FastAPI/LangGraph infrastructure
- Must maintain streaming endpoint behavior
- Must keep observability/logging working
- Must deploy to Vercel (serverless functions)
- Environment variables are the configuration mechanism

---

## 17. Questions to Ask User (If Needed)

If anything is unclear, consider asking:

1. "What should we name the new repository?" (if not decided)
2. "Do you want to keep the PMM domain working, or fully replace it?" (we're keeping it)
3. "Should we test after each major step, or convert everything first?" (incremental preferred)
4. "Do you have the `DOMAIN_CONVERSION_GAMEPLAN.md` file copied to the new repo?" (they will)
5. "Are you ready to start the conversion, or need help with setup first?" (they want setup help)

---

## 18. Final Reminders

‚úÖ **Read `DOMAIN_CONVERSION_GAMEPLAN.md`** - It's the master plan  
‚úÖ **Follow `docs/CUSTOMIZATION.md`** - Official 4-step process  
‚úÖ **Reference existing PMM code** - It's your template  
‚úÖ **Test incrementally** - Don't convert everything at once  
‚úÖ **Keep it simple** - MVP first, refine later  
‚úÖ **Maintain compatibility** - PMM domain should still work  
‚úÖ **Ask if unclear** - User values clarity  

**You've got this! The gameplan is comprehensive, the patterns are clear, and the user is ready to proceed.** üöÄ

---

**Document prepared:** December 26, 2025  
**For:** Next AI Agent Session  
**Purpose:** Seamless handoff and context transfer

